{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SojiroNishimura/deeplearning-from-scratch/blob/master/chapter5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I70b_VjXgm4x",
        "colab_type": "text"
      },
      "source": [
        "# 誤差逆伝播法\n",
        "ニューラルネットワークの学習を行うためには各層での勾配を求める必要がある。勾配は損失関数を数値微分することで求められるが、数値微分の計算は時間がかかる。誤差逆伝播法を使うことで各時点における出力を解析的に微分することができるため、効率的（=高速）に学習が完了する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80aem4ItkVU1",
        "colab_type": "text"
      },
      "source": [
        "## 連鎖律\n",
        "ある計算をグラフで表現した場合、左(始点)から右(終点)に向けて計算結果を伝達させていくことを**順伝播**という。逆に右(終点)から左(始点)に向けて、入力と各ノードにおける局所的な微分の値を掛け合わせていくことを**逆伝播**という。\n",
        "\n",
        "$y=f(x)$という計算の場合、逆伝播の入力を$E$とすると$E$とノードの計算を微分したもの=$\\frac{\\partial y}{\\partial x}$を掛け合わせた$E\\frac{\\partial y}{\\partial x}$がそのノードの逆伝播の出力となり、次の(1つ前の）ノードへの入力となる。\n",
        "\n",
        "### 合成関数\n",
        "合成関数=複数の関数によって構成される関数。例として$z=(x+y)^2$は以下の2式で構成される。\n",
        "\n",
        "$z = t^2$\n",
        "\n",
        "$t = x + y$\n",
        "\n",
        "連鎖律は以下のように定義される。\n",
        "\n",
        "> ある関数が合成関数で表される場合、その合成関数の微分は**合成関数を構成するそれぞれの関数の微分の積**によって表すことができる。\n",
        "\n",
        "上記の式に当てはめると以下のようになる。\n",
        "\n",
        "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t}\\frac{\\partial t}{\\partial x} = \\frac{\\partial z}{\\partial x}$$\n",
        "\n",
        "先ほどの式に適用すると以下のとおり。\n",
        "\n",
        "$$\\frac{\\partial z}{\\partial t} = 2t$$\n",
        "\n",
        "$$\\frac{\\partial t}{\\partial x} = 1$$\n",
        "\n",
        "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t}\\frac{\\partial t}{\\partial x} = 2t\\cdot1 = 2(x + y)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyPJ_Ad_00YO",
        "colab_type": "text"
      },
      "source": [
        "## 逆伝播\n",
        "「加算」「乗算」の各計算の逆伝播。\n",
        "\n",
        "### 加算ノードの逆伝播\n",
        "$z = x + y$の場合\n",
        "\n",
        "$$\\frac{\\partial z}{\\partial x} = 1$$\\\n",
        "$$\\frac{\\partial z}{\\partial y} = 1$$\n",
        "\n",
        "右側(上流)からの入力が$\\frac{\\partial L}{\\partial z}$とすると、加算ノードにおける逆伝播の出力はそれぞれ$\\frac{\\partial L}{\\partial z}\\cdot 1$となる。\n",
        "\n",
        "### 乗算ノードの逆伝播\n",
        "$z = xy$の場合\n",
        "\n",
        "$$\\frac{\\partial z}{\\partial x} = y$$\n",
        "\n",
        "$$\\frac{\\partial z}{\\partial y} = x$$\n",
        "\n",
        "右側(上流)からの入力が$\\frac{\\partial L}{\\partial z}$とすると、逆伝播の出力はそれぞれ以下のようになる。\n",
        "\n",
        "x方向への逆伝播：$\\frac{\\partial L}{\\partial z}\\cdot y$\n",
        "\n",
        "y方向への逆伝播：$\\frac{\\partial L}{\\partial z}\\cdot x$\n",
        "\n",
        "各ノードは以下のように実装できる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lqN_amfYyLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 乗算レイヤ\n",
        "class MulLayer:\n",
        "  def __init__(self):\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "    \n",
        "  def forward(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    out = x * y\n",
        "    \n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dx = dout * self.y # xとyをひっくり返す\n",
        "    dy = dout * self.x\n",
        "    \n",
        "    return dx, dy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG0lqY706LzS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad1c9ed4-5327-41bb-b6ce-951ad5fa8bd1"
      },
      "source": [
        "# 消費税１０%で1個110円のりんごを2個買う場合の順伝播での計算\n",
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "# layer\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "print(price)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "220.00000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHSZdxQP7G6Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4005b3bc-5ede-4194-f315-2e2f58472f40"
      },
      "source": [
        "# 各変数の微分(backward)\n",
        "dprice = 1\n",
        "\n",
        "# 順伝播の時とは逆向きに計算を行う\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(dapple, dapple_num, dtax)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2 110.00000000000001 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXxjif_478Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 加算レイヤ\n",
        "class AddLayer:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    out = x + y\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dx = dout * 1\n",
        "    dy = dout * 1\n",
        "    return dx, dy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCsctTvW8hwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c8f1f189-a8b5-4dce-95e3-61ae0d8c18e7"
      },
      "source": [
        "# 消費税10%で100円のりんご2個と150円のみかん3個を買う場合の順伝播での計算\n",
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# layer\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_apple_orange_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num) # (1)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num) # (2)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price) # (3)\n",
        "price = mul_tax_layer.forward(all_price, tax) # (4)\n",
        "\n",
        "# backward\n",
        "dprice = 1\n",
        "dall_price, dtax = mul_tax_layer.backward(dprice) # (4)\n",
        "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) # (3)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price) # (2)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price) # (1)\n",
        "\n",
        "print(price)\n",
        "print(dapple_num, dapple, dorange, dorange_num, dtax)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "715.0000000000001\n",
            "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcN5xjPY4x41",
        "colab_type": "text"
      },
      "source": [
        "## 活性化関数レイヤの実装\n",
        "活性化関数は各ニューロンへの入力を引数にとり出力に変換する。活性化関数は隠れ層に内包されていると見ることもできるが、ここでは活性化関数も1つの層(レイヤ)とみなす。\n",
        "\n",
        "### ReLUレイヤ\n",
        "よく使われる活性化関数として **ReLU(RectifiedL Linear Unit)** がある。これは入力が0以下なら0を出力し、0より大きければ入力値をそのまま出力する。\n",
        "\n",
        "$\n",
        "\\begin{eqnarray}\n",
        "y = \\left\\{\n",
        "  \\begin{array}{ll}\n",
        "    x ~~~ (x > 0)\\\\\n",
        "    0 ~~~ (x \\le 0)\n",
        "  \\end{array} \\right.\n",
        "\\end{eqnarray}\n",
        "$\n",
        "\n",
        "ReLUを微分すると以下のようになる。\n",
        "\n",
        "$\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial y}{\\partial x} = \\left\\{\n",
        "  \\begin{array}{ll}\n",
        "    1 ~~~ (x > 0)\\\\\n",
        "    0 ~~~ (x \\le 0)\n",
        "  \\end{array} \\right.\n",
        "\\end{eqnarray}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2t_xjWB-SIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Relu:\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    self.mask = (x <= 0)\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0\n",
        "    \n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0\n",
        "    dx = dout\n",
        "    \n",
        "    return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf43tdHe9a7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "861e003a-4c74-47cf-e262-b4fc7da1ee73"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
        "print(x)\n",
        "\n",
        "mask = (x <= 0)\n",
        "print(mask)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.  -0.5]\n",
            " [-2.   3. ]]\n",
            "[[False  True]\n",
            " [ True False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrhH6eMI96mm",
        "colab_type": "text"
      },
      "source": [
        "### Sigmoidレイヤ\n",
        "シグモイド関数は以下の式で表される。\n",
        "\n",
        "$y = \\frac{1}{1 + exp(-x)}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZIC2Jsc9kWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = 1 / (1 + np.exp(-x))\n",
        "    self.out = out\n",
        "    \n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "    return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJk1zmFO_G-y",
        "colab_type": "text"
      },
      "source": [
        "## Affine/Softmaxレイヤの実装\n",
        "### Affineレイヤ\n",
        "ニューラルネットワークの順伝播で行う行列の積を幾何学の分野で **アフィン変換** と呼ぶ。アフィン変換を行うレイヤ(=順伝播の処理)をAffineレイヤとして定義する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKY_NAL_-900",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.W = W # 重み\n",
        "    self.b = b # バイアス\n",
        "    self.x = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    self.x = x\n",
        "    out = np.dot(x, self.W) + self.b\n",
        "    \n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dx = np.dot(dout, self.W.T) # 逆伝播の時は入力に重み行列を転置して掛け合わせる\n",
        "    self.dW = np.dot(self.x.T, dout)\n",
        "    self.db = np.sum(dout, axis=0)\n",
        "    \n",
        "    return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGOpzgQaBq_F",
        "colab_type": "text"
      },
      "source": [
        "### Softmax-with-Lossレイヤ\n",
        "ソフトマックス関数は出力値を正規化し、出力値の合計が1になるように出力する。なお学習の際は最終的な出力を行う際にソフトマックス関数を実行しなければならないが、推論を行う際は実行しなくてもよい。これは最終出力直前時点での最大値がソフトマックス関数を実行しても最大であることは自明だからである。なお最終出力直前(正規化しない)の値を **「スコア」** と呼ぶことがある。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl7rG7Agd3un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x) # オーバーフロー対策\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "  \n",
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "      t = t.reshape(1, t.size)\n",
        "      y = y.reshape(1, y.size)\n",
        "\n",
        "  # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "  if t.size == y.size:\n",
        "      t = t.argmax(axis=1)\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S79PUfglBnc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None # 損失\n",
        "    self.y = None # softmaxの出力\n",
        "    self.t = None # 教師データ(one-hot vector)\n",
        "    \n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(x)\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\n",
        "    \n",
        "    return self.loss\n",
        "  \n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    dx = (self.y - self.t) / batch_size\n",
        "    \n",
        "    return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJuaTqcyeUO9",
        "colab_type": "text"
      },
      "source": [
        "## 誤差逆伝播法の実装\n",
        "ニューラルネットワークの学習は以下の4ステップで行う。\n",
        "\n",
        "1. ミニバッチ\n",
        "  * 訓練データからランダムに一部を選び出す\n",
        "2. 勾配の算出\n",
        "  * 各重みパラメータに関する損失関数の勾配を求める\n",
        "  * 勾配=損失関数を偏微分した結果のベクトル\n",
        "3. パラメータの更新\n",
        "  * 重みパラメータを勾配方向に微小量だけ更新する\n",
        "4. 繰り返す\n",
        "  * 値が収束するor一定回数1〜3を繰り返す\n",
        "  \n",
        "誤差逆伝播法は2の勾配の算出において、数値微分より高速に勾配を計算するために使用する。誤差逆伝播法では、数値微分を行うのではなく解析的に解くことによって高速に計算を行うことができる。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBckONo6lGGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        \n",
        "        x[idx] = tmp_val # 値を元に戻す\n",
        "        it.iternext()   \n",
        "        \n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AP7cwKxgANH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "    # 重みの初期化\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "    \n",
        "    # レイヤの生成\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "    \n",
        "    self.lastLayer = SoftmaxWithLoss()\n",
        "    \n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "      \n",
        "    return x\n",
        "  \n",
        "  # x:入力データ, t:教師データ\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.lastLayer.forward(y, t)\n",
        "  \n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "      \n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "  \n",
        "  # x:入力データ, t:教師データ\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "    \n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "    \n",
        "    return grads\n",
        "  \n",
        "  def gradient(self, x, t):\n",
        "    # forward\n",
        "    self.loss(x, t)\n",
        "    \n",
        "    # backward\n",
        "    dout = 1\n",
        "    dout = self.lastLayer.backward(dout)\n",
        "    \n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "      \n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Affine1'].dW\n",
        "    grads['b1'] = self.layers['Affine1'].db\n",
        "    grads['W2'] = self.layers['Affine2'].dW\n",
        "    grads['b2'] = self.layers['Affine2'].db\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh10mqkMl9AU",
        "colab_type": "text"
      },
      "source": [
        "### 勾配確認\n",
        "数値微分と誤差逆伝播法での解析的な解法では後者の方が効率的で高速に勾配の計算ができるが、数値微分に比べて実装が複雑になる。数値微分と誤差逆伝播法の計算結果を比較する(両者の差が非常に小さいことを確認する)ことで、誤差逆伝播法の実装に問題がないか確認することができる。これを **勾配確認(gradient check)** と言う。\n",
        "\n",
        "(実装は割愛)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxKVJk0MpEwG",
        "colab_type": "text"
      },
      "source": [
        "### 誤差逆伝播法を使った学習\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xzjMsgnnKTK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a190d3a9-dc20-491c-c7eb-2c01fac4fee4"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "t_train = np_utils.to_categorical(t_train)\n",
        "t_test = np_utils.to_categorical(t_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjr1IcaWpJP7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "5297e55d-428d-429b-fce2-e6388d4067b8"
      },
      "source": [
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "  \n",
        "  # 誤差逆伝播法によって勾配を求める\n",
        "  grad = network.gradient(x_batch, t_batch)\n",
        "  \n",
        "  # 更新\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "  \n",
        "  if i % iter_per_epoch == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(train_acc, test_acc)\n",
        "    \n",
        "print(\"End\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09751666666666667 0.0974\n",
            "0.11241666666666666 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "0.11236666666666667 0.1135\n",
            "End\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNKMpLxAr0cI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}